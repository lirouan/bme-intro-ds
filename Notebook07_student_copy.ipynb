{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLzIWpl8i5ys"
   },
   "source": [
    "# Prediction of Liver Disease [117 points total]\n",
    "\n",
    "**Liver Disease (LD)** is a serious condition in which the liver’s ability to detoxify, metabolize, and synthesize proteins is impaired. In this pre-lab you will use clinical features to **predict the presence of liver disease**. You will build two models:\n",
    "\n",
    "- **K-Nearest Neighbors (K-NN)** with feature scaling  \n",
    "- **Random Forest** (no scaling needed)\n",
    "\n",
    "> **Provided:** The dataset is already loaded for you as a pandas DataFrame named `df`.  \n",
    "> **Target column:** `disease` (binary: `1` = liver disease, `0` = healthy).  \n",
    "> **Features:** numeric only. Covert Male_Gender to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "FILENAME   = \"ilpd_clean.csv\"\n",
    "LOCAL_DIR  = None  # e.g. r\"/Users/you/data/ilpd_release\" (leave as None to use current folder)\n",
    "\n",
    "try:\n",
    "    # --- Colab case ---\n",
    "    import google.colab  # type: ignore\n",
    "    from google.colab import drive  # type: ignore\n",
    "\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "    base = Path(\"/content/drive/MyDrive\")\n",
    "    csv_path = (base / \"ilpd_release\" / FILENAME)\n",
    "    if not csv_path.exists():\n",
    "        csv_path = base / FILENAME  # fall back to MyDrive root\n",
    "except ImportError:\n",
    "    # --- Local case ---\n",
    "    if LOCAL_DIR:\n",
    "        base = Path(LOCAL_DIR).expanduser()\n",
    "        csv_path = base / FILENAME if base.is_dir() else Path(LOCAL_DIR)\n",
    "    else:\n",
    "        csv_path = Path.cwd() / FILENAME  # assume file is next to the notebook\n",
    "\n",
    "# Final load\n",
    "if not csv_path.exists():\n",
    "    raise FileNotFoundError(f\"Couldn't find {FILENAME} at: {csv_path}\\n\"\n",
    "                            f\"Put the file next to the notebook, or set LOCAL_DIR to its folder.\")\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 — Set up the Features and Target *(10 points)*\n",
    "\n",
    "**Goal.** Identify the target (`disease`) and define the feature set for modeling (numeric features only).\n",
    "\n",
    "**What you should do (conceptually):**\n",
    "- Verify the dataset’s basic structure (shape, first few rows, data types).\n",
    "- Confirm the target column is `disease` and encoded as **0/1** integers.\n",
    "- Define the feature columns as **all non-target** variables.\n",
    "- If a boolean indicator like `Gender_Male` exists, ensure it’s represented as **0/1** (without altering non-boolean values).\n",
    "\n",
    "**Checklist for completion:**\n",
    "- Target column correctly identified and encoded as 0/1.\n",
    "- Feature set excludes the target and contains other variables including gender.\n",
    "- Any boolean indicators are consistently represented as 0/1.\n",
    "- Brief note on whether any missing values are present.\n",
    "\n",
    "**Grading (10 points):**\n",
    "- Target identification & encoding (0–3)\n",
    "- Proper feature selection (target excluded) (0–4)\n",
    "- Boolean handling for Gender feature (0–2)\n",
    "- Brief data sanity checks (shape/dtypes/any missing vlues) (0–1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.api.types import is_bool_dtype\n",
    "\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, f1_score, classification_report,\n",
    "    ConfusionMatrixDisplay,precision_recall_curve, average_precision_score, RocCurveDisplay\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Set up the features and target\n",
    "# ----------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 — Train / Test Split *(10 points)*\n",
    "\n",
    "**Goal.** Create a stratified train/test split that preserves class proportions and keeps the test set untouched for final evaluation.\n",
    "\n",
    "**What you should do (conceptually):**\n",
    "- Define **X** as the selected numeric feature matrix and **y** as the `disease` label.\n",
    "- Perform an **75/25 split** **with stratification** on `y` to maintain class balance.\n",
    "- Use a **fixed random seed** for reproducibility (42).\n",
    "- Keep the **test set strictly untouched** until the final evaluation.\n",
    "\n",
    "**Checklist for completion:**\n",
    "- Clear identification of X (features) and y (target).\n",
    "- Stratified split performed with the specified test size.\n",
    "- Random seed documented and used.\n",
    "- Class balance verified (at least on the full set; ideally also on train vs. test).\n",
    "- Statement that the test set will not be used during model selection/tuning.\n",
    "\n",
    "**Grading (10 points):**\n",
    "- Correct definition of X and y (0–2)\n",
    "- Proper **stratified** split at the requested ratio (0–4)\n",
    "- Reproducibility via fixed random state (0–2)\n",
    "- Class balance check & “no peeking” note for the test set (0–2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2) Train / Test split\n",
    "# ----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 — 5-Fold CV + Grid Search for K-NN *(12 points)*\n",
    "\n",
    "**Goal.** Select the best **K-NN** hyperparameters using **5-fold Stratified cross-validation**:\n",
    "- Tune **k** (number of neighbors) over a small, odd-valued set.\n",
    "- Compare **distance weighting**: `uniform` vs. `distance`.\n",
    "- Use **`f1_macro`** as the primary scoring metric (treats classes evenly).\n",
    "\n",
    "**What you should do (conceptually):**\n",
    "- Build a pipeline that standardizes features and applies K-NN.\n",
    "- Define a **grid** for `k` (e.g., 3–17, odd values) and `weights` (`uniform`, `distance`).\n",
    "- Use **Stratified 5-fold CV**, shuffled with a fixed random seed for reproducibility.\n",
    "- Run a **grid search**, select the configuration with the best mean **`f1_macro`**.\n",
    "- Refit the **best model on the full training set** (standard practice after selection).\n",
    "\n",
    "**Checklist for completion:**\n",
    "- Pipeline includes **scaling → K-NN**.\n",
    "- Hyperparameter grid covers **k** and **weights** clearly.\n",
    "- CV is **Stratified**, **5 folds**, **shuffled**, **seeded**.\n",
    "- Primary scoring is **`f1_macro`** (with brief rationale).\n",
    "- Best model **refit** on train.\n",
    "\n",
    "**Grading (12 points):**\n",
    "- proper pipeline (scaling and K-NN) (0-2)\n",
    "- Correct CV design (Stratified, 5-fold, shuffled, seeded) (0–4)\n",
    "- Sensible hyperparameter grid (k range + weights) (0–4)\n",
    "- Proper scoring choice and justification (`f1_macro`) (0–3)\n",
    "- Best model refit on the full training set (0–1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3) Use 5-fold CV with grid search to identify best \n",
    "# Knn parameter ( k value and distance weighting \"uniform\", \"distance\")\n",
    "# ----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 — Apply Best Model to Test Data *(5 points)*\n",
    "\n",
    "**Goal.** Use the **refit best estimator** from the grid search to generate **test-set predictions**.\n",
    "\n",
    "**What you should do (as used here):**\n",
    "- Retrieve the refit best pipeline/estimator from the search object.\n",
    "- Report the **best hyperparameters**.\n",
    "- Produce **`y_pred`** by predicting on **`X_test`**.\n",
    "\n",
    "**Checklist for completion:**\n",
    "- Best estimator retrieved and identified.\n",
    "- Best parameters reported.\n",
    "- Test-set predictions (`y_pred`) generated from the best estimator.\n",
    "\n",
    "**Grading (5 points):**\n",
    "- Retrieve refit best estimator (0–2)  \n",
    "- Report best params (0–1)  \n",
    "- Predict on `X_test` to obtain `y_pred` (0–2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4) Apply on Test data to get Y_pred   \n",
    "# ----------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 — Class Evaluation Table & Confusion Matrix *(10 points)*\n",
    "\n",
    "**Goal.** Summarize test-set performance with a **per-class metrics table** and a **confusion matrix**.\n",
    "\n",
    "**What you should do (as used here):**\n",
    "- Generate a **classification report** showing **precision, recall, F1-score, support** for each class.\n",
    "- Plot a **confusion matrix** using the test labels and predictions.\n",
    "- Provide a short interpretation (e.g., which class is misclassified more often).\n",
    "\n",
    "**Checklist for completion:**\n",
    "- Classification report produced for the **test set** (per-class metrics visible).\n",
    "- Confusion matrix plotted for **`y_test` vs. `y_pred`**.\n",
    "- Clear title/labels and a brief interpretation of errors.\n",
    "\n",
    "**Grading (10 points):**\n",
    "- Correct classification report on test set (0–4)  \n",
    "- Correct confusion matrix on test set (0–4)  \n",
    "- Brief, accurate interpretation (0–2)\n",
    "\n",
    "- **Provide a short interpretation:** <mark>Which class is misclassified more often (describe your interpretation)?</mark>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5) Class Evaluation Table and Confusion Matrix \n",
    "# ----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 — Visualize ROC & Precision–Recall *(10 points)*\n",
    "\n",
    "**Goal.** For binary classification, summarize threshold-independent performance with **ROC** and **Precision–Recall** visualizations.\n",
    "\n",
    "**What you should do (as used here):**\n",
    "- Obtain **predicted probabilities** for the positive class (if the model provides them).\n",
    "- Compute and **report ROC AUC** on the test set.\n",
    "- Plot the **Precision–Recall (PR) curve** and display **Average Precision (AP)**.\n",
    "- Plot the **ROC curve** for the test set.\n",
    "\n",
    "**Checklist for completion:**\n",
    "- Positive-class probabilities obtained for the test set.\n",
    "- **ROC AUC** reported.\n",
    "- **PR curve** plotted with **AP** labeled.\n",
    "- **ROC curve** plotted with an appropriate title.\n",
    "\n",
    "**Grading (10 points):**\n",
    "- Correct probability extraction for the positive class (0–3)  \n",
    "- ROC AUC computed and reported (0–2)  \n",
    "- PR curve + AP shown (0–3)  \n",
    "- ROC curve shown (0–2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6) Visualize ROC and Precision–Recall\n",
    "# ----------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7 — Interpret Your Results *(10 points)*\n",
    "\n",
    "**Goal.** Demonstrate understanding of the evaluation metrics/plots without repeating earlier confusion-matrix analysis.\n",
    "\n",
    "**Answer briefly (1–2 sentences each):**\n",
    "1. **Primary metric choice:** Given the ~71/29 class mix, explain why **F1-macro** (or balanced accuracy) is preferable to plain accuracy.\n",
    "- **Provide your answer:** <mark>Question 1?</mark>\n",
    " \n",
    "2. **ROC-AUC vs. PR (AP):** Which is more informative for this dataset and why?\n",
    "- **Provide your answer:** <mark>Question 2?</mark>\n",
    "\n",
    "\n",
    " \n",
    "3. **Thresholding:** If prioritizing recall for disease, would you raise or lower the 0.5 threshold, and what trade-off do you expect?\n",
    "- **Provide your answer:** <mark>Question 3?</mark>\n",
    "\n",
    "\n",
    "**Grading (10 points):**\n",
    "- Metric choice justification (0–4)  \n",
    "- ROC-AUC vs. PR insight (0–3)  \n",
    "- Threshold trade-off explanation (0–3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8 — 5-Fold CV + Grid Search for Random Forest *(15 points)*\n",
    "\n",
    "**Goal.** Use **5-fold Stratified cross-validation** to select Random Forest hyperparameters and refit the best model.\n",
    "\n",
    "**What you should do (as used here):**\n",
    "- Build a pipeline with a **RandomForestClassifier**.\n",
    "- Define **StratifiedKFold(n_splits=5, shuffle=True, random_state=42)**.\n",
    "- Specify the hyperparameter grid:\n",
    "  - **n_estimators:** {200, 400}\n",
    "  - **max_depth:** {None, 10, 20}\n",
    "  - **min_samples_split:** {2, 10}\n",
    "  - **min_samples_leaf:** {1, 5}\n",
    "  - **max_features:** {\"sqrt\", 0.5}\n",
    "  - Run a **grid search**, select the configuration with the best mean **`f1_macro`**.\n",
    "  - Refit the **best model on the full training set** (standard practice after selection).\n",
    "\n",
    "\n",
    "\n",
    "**Checklist for completion:**\n",
    "- RF pipeline defined.\n",
    "- Stratified 5-fold CV (shuffled, seeded) used.\n",
    "- Grid exactly as listed above.\n",
    "- Best model refit on training data.\n",
    "\n",
    "**Grading (15 points):**\n",
    "- Correct CV setup (Stratified, 5-fold, shuffle, seed) (0–4)\n",
    "- Hyperparameter grid exactly matches the spec (0–4)\n",
    "- Proper Grid Search (0–4)\n",
    "- Best-model refit stated (0–3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8) Use 5-fold CV with grid search to identify best Random Forest parameter\n",
    "# \n",
    "# n_estimators\": [200, 400]\n",
    "# max_depth\": [None, 10, 20]\n",
    "# min_samples_split\": [2, 10]\n",
    "# min_samples_leaf\": [1, 5]\n",
    "# max_features\": [\"sqrt\", 0.5]\n",
    "# ----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9 — Apply Best Model to the Test Set *(5 points)*\n",
    "\n",
    "**Goal.** Use the refit **best model** (selected by cross-validation) to generate **predictions on the held-out test set**.\n",
    "\n",
    "**What you should do (conceptually):**\n",
    "- Retrieve the **best-performing** model from your hyperparameter search (the one automatically refit on the full training data).\n",
    "- Note the **best hyperparameters** selected (for reporting).\n",
    "- Produce **test-set predictions** using the untouched **X_test**.\n",
    "\n",
    "**Checklist for completion:**\n",
    "- Best model clearly identified and referenced.\n",
    "- Best hyperparameters briefly reported.\n",
    "- Test-set predictions generated (and stored) for downstream evaluation.\n",
    "\n",
    "**Grading (5 points):**\n",
    "- Identify and use the refit best model (0–2)  \n",
    "- Report best hyperparameters (0–1)  \n",
    "- Generate predictions on the test set (0–2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 9) Apply on Test data to get Y_predict \n",
    "# ----------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10 — Class Evaluation Table & Confusion Matrix *(10 points)*\n",
    "\n",
    "**Goal.** Summarize **test-set** performance of your **Random Forest** with a per-class metrics table and a confusion matrix.\n",
    "\n",
    "**What you should do (conceptually):**\n",
    "- Produce a **classification report** on the test set showing **precision, recall, F1-score, support** for each class.\n",
    "- Plot a **confusion matrix** comparing **true labels vs. predictions**.\n",
    "- Provide a **brief interpretation**: which class is misclassified more often, and what that implies (e.g., lower recall for the minority class).\n",
    "\n",
    "**Checklist for completion:**\n",
    "- Classification report generated for the **test set**.\n",
    "- Confusion matrix plotted and clearly labeled (title reflects Random Forest).\n",
    "- One–two sentence interpretation referencing a specific metric or cell in the matrix.\n",
    "\n",
    "**Grading (10 points):**\n",
    "- Correct classification report on test set (0–4)  \n",
    "- Correct confusion matrix on test set (0–4)  \n",
    "- Clear, concise interpretation (0–2)\n",
    "\n",
    "- **Provide a short interpretation:** <mark>Which class is misclassified more often (describe your interpretation)?</mark>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 10) Class Evaluation Table and Confusion Matrix \n",
    "# ----------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11 — Visualize ROC & Precision–Recall *(10 points)*\n",
    "\n",
    "**Goal.** Use **threshold-independent** plots to evaluate your **Random Forest** on the test set.\n",
    "\n",
    "**What you should do (conceptually):**\n",
    "- Obtain **predicted probabilities** for the **positive** class from the best model.\n",
    "- Compute and **report ROC-AUC** on the test set.\n",
    "- Plot the **Precision–Recall (PR) curve** and report **Average Precision (AP)**.\n",
    "- Plot the **ROC curve** and label axes and title appropriately.\n",
    "\n",
    "**Checklist for completion:**\n",
    "- Positive-class probabilities extracted for **X_test**.\n",
    "- **ROC-AUC** reported (test set).\n",
    "- **PR curve** shown with **AP** displayed.\n",
    "- **ROC curve** shown with clear title/labels referencing Random Forest.\n",
    "\n",
    "**Grading (10 points):**\n",
    "- Correct probability extraction for the positive class (0–3)  \n",
    "- ROC-AUC computed and reported (0–2)  \n",
    "- PR curve + AP displayed (0–3)  \n",
    "- ROC curve displayed (0–2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 11) Visualize ROC and Precision–Recall\n",
    "# ----------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12 — Top Features by Importance *(10 points)*\n",
    "\n",
    "**Goal.** Identify and visualize the **top 10 features** that contribute most to the **Random Forest** model.\n",
    "\n",
    "**What you should do (conceptually):**\n",
    "- Obtain the **feature importance scores** from the trained Random Forest.\n",
    "- Align these scores with the **exact feature names** used to fit the model.\n",
    "- Select the **top 10** features by importance and **order them descending** (largest at the top).\n",
    "- Create a **horizontal bar chart** of these top features.\n",
    "- Add a concise caption/observation about which variables dominate and any domain-plausible reasons.\n",
    "\n",
    "**Checklist for completion:**\n",
    "- Correct mapping: importance scores ↔ feature names.\n",
    "- Top **10** features selected and sorted (highest to lowest).\n",
    "- Horizontal bar plot with **clear labels** (feature names on the y-axis, “Feature importance” on the x-axis).\n",
    "- Brief interpretation (1–2 sentences).\n",
    "\n",
    "**Grading (10 points):**\n",
    "- Proper name–importance alignment (0–3)  \n",
    "- Correct selection/sorting of top 10 (0–3)  \n",
    "- Clear, labeled plot (0–2)  \n",
    "- Brief interpretation (0–2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 12) Get and plot the top 10 features ranked in descending order according to importance\n",
    "# -----------------------------\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
